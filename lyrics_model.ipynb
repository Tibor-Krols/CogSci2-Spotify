{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Lyrics Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_cleaned_sentiment_train.csv', delimiter = ',')\n",
    "data_test = pd.read_csv('merged_cleaned_sentiment_test.csv', delimiter = ',')\n",
    "data_val = pd.read_csv('merged_cleaned_sentiment_validation.csv', delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data[['pos','neg','neu','compound']]\n",
    "data_labels = data[['y_valence', 'y_arousal']]\n",
    "\n",
    "data_features_test = data_test[['pos','neg','neu','compound']]\n",
    "data_labels_test = data_test[['y_valence', 'y_arousal']]\n",
    "\n",
    "data_features_val = data_val[['pos','neg','neu','compound']]\n",
    "data_labels_val = data_val[['y_valence', 'y_arousal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train_vader, y_train = np.array(data_features), np.array(data_labels)\n",
    "X_test_vader, y_test = np.array(data_features_test), np.array(data_labels_test)\n",
    "X_val_vader, y_val = np.array(data_features_val), np.array(data_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816597264467664\n",
      "0.022844544146133294\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train_vader,y_train)\n",
    "prediction = reg.predict(X_val_vader)\n",
    "print(mean_squared_error(y_val, prediction, squared = False))\n",
    "print(r2_score(y_val, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very high RMSE considering our predicted values go from -1 to 1. The RMSE is half of the range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics_train = np.array(data['lyrics_cleaned'])\n",
    "data_lyrics_test = np.array(data_test['lyrics_cleaned'])\n",
    "data_lyrics_val = np.array(data_val['lyrics_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up the lyrics texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data): \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_data = np.full((data.shape), None)\n",
    "\n",
    "    for n, lyrics in enumerate(data):\n",
    "        lyrics = [word.lower().strip(string.punctuation) for word in lyrics.split()]\n",
    "        lyrics = [lemmatizer.lemmatize(word) for word in lyrics]\n",
    "        clean_data[n] = ' '.join(lyrics)\n",
    "\n",
    "    return clean_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = preprocessing(data_lyrics_train)\n",
    "clean_test = preprocessing(data_lyrics_test)\n",
    "clean_val = preprocessing(data_lyrics_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating TF-IDF Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'word', ngram_range = (1,1))\n",
    "X_train_tfidf = vectorizer.fit_transform(clean_train)\n",
    "X_test_tfidf = vectorizer.transform(clean_test)\n",
    "X_val_tfidf = vectorizer.transform(clean_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.26271753604112447\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 100)\n",
    "pca.fit(X_train_tfidf.toarray().astype(float))\n",
    "pca_lyrics_train = pca.transform(X_train_tfidf.toarray().astype(float))\n",
    "pca_lyrics_test = pca.transform(X_test_tfidf.toarray().astype(float))\n",
    "pca_lyrics_val = pca.transform(X_val_tfidf.toarray().astype(float))\n",
    "print('Explained variance:', np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANEW Count Features - lexical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "GitCommandError",
     "evalue": "Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v https://github.com/JULIELab/X-ANEW.git /Users/yananikolova/Documents/GitHub/CogSci2-Spotify/anew\n  stderr: 'fatal: destination path '/Users/yananikolova/Documents/GitHub/CogSci2-Spotify/anew' already exists and is not an empty directory.\n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGitCommandError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/yananikolova/Documents/GitHub/CogSci2-Spotify/lyrics_model.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yananikolova/Documents/GitHub/CogSci2-Spotify/lyrics_model.ipynb#ch0000034?line=0'>1</a>\u001b[0m Repo\u001b[39m.\u001b[39;49mclone_from(\u001b[39m'\u001b[39;49m\u001b[39mhttps://github.com/JULIELab/X-ANEW.git\u001b[39;49m\u001b[39m'\u001b[39;49m, Path\u001b[39m.\u001b[39;49mhome()\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mDocuments/GitHub/CogSci2-Spotify/anew\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py:1111\u001b[0m, in \u001b[0;36mRepo.clone_from\u001b[0;34m(cls, url, to_path, progress, env, multi_options, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1108'>1109</a>\u001b[0m \u001b[39mif\u001b[39;00m env \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1109'>1110</a>\u001b[0m     git\u001b[39m.\u001b[39mupdate_environment(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39menv)\n\u001b[0;32m-> <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1110'>1111</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py:1049\u001b[0m, in \u001b[0;36mRepo._clone\u001b[0;34m(cls, git, url, path, odb_default_type, progress, multi_options, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1045'>1046</a>\u001b[0m     cmdline \u001b[39m=\u001b[39m remove_password_if_present(cmdline)\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1047'>1048</a>\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCmd(\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms unused stdout: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, cmdline, stdout)\n\u001b[0;32m-> <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1048'>1049</a>\u001b[0m     finalize_process(proc, stderr\u001b[39m=\u001b[39;49mstderr)\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1050'>1051</a>\u001b[0m \u001b[39m# our git command could have a different working dir than our actual\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1051'>1052</a>\u001b[0m \u001b[39m# environment, hence we prepend its working dir if required\u001b[39;00m\n\u001b[1;32m   <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/repo/base.py?line=1052'>1053</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m osp\u001b[39m.\u001b[39misabs(path):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/git/util.py:370\u001b[0m, in \u001b[0;36mfinalize_process\u001b[0;34m(proc, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/util.py?line=367'>368</a>\u001b[0m \u001b[39m\"\"\"Wait for the process (clone, fetch, pull or push) and handle its errors accordingly\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/util.py?line=368'>369</a>\u001b[0m \u001b[39m## TODO: No close proc-streams??\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/util.py?line=369'>370</a>\u001b[0m proc\u001b[39m.\u001b[39;49mwait(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/git/cmd.py:447\u001b[0m, in \u001b[0;36mGit.AutoInterrupt.wait\u001b[0;34m(self, stderr)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/cmd.py?line=444'>445</a>\u001b[0m         errstr \u001b[39m=\u001b[39m read_all_from_possibly_closed_stream(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproc\u001b[39m.\u001b[39mstderr)\n\u001b[1;32m    <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/cmd.py?line=445'>446</a>\u001b[0m         log\u001b[39m.\u001b[39mdebug(\u001b[39m'\u001b[39m\u001b[39mAutoInterrupt wait stderr: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (errstr,))\n\u001b[0;32m--> <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/cmd.py?line=446'>447</a>\u001b[0m         \u001b[39mraise\u001b[39;00m GitCommandError(remove_password_if_present(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs), status, errstr)\n\u001b[1;32m    <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/cmd.py?line=447'>448</a>\u001b[0m \u001b[39m# END status handling\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/yananikolova/opt/anaconda3/lib/python3.9/site-packages/git/cmd.py?line=448'>449</a>\u001b[0m \u001b[39mreturn\u001b[39;00m status\n",
      "\u001b[0;31mGitCommandError\u001b[0m: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v https://github.com/JULIELab/X-ANEW.git /Users/yananikolova/Documents/GitHub/CogSci2-Spotify/anew\n  stderr: 'fatal: destination path '/Users/yananikolova/Documents/GitHub/CogSci2-Spotify/anew' already exists and is not an empty directory.\n'"
     ]
    }
   ],
   "source": [
    "#Repo.clone_from('https://github.com/JULIELab/X-ANEW.git', Path.home()/'Documents/GitHub/CogSci2-Spotify/anew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xanew = pd.read_csv('anew/Ratings_Warriner_et_al.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xanew = xanew[['Word', 'V.Mean.Sum', 'A.Mean.Sum']]\n",
    "xanew.columns = ['word', 'valence', 'arousal']\n",
    "xanew.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewords = [str(el) for el in list(xanew.index)]\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary = ewords)\n",
    "X_train_anew = vectorizer.transform(clean_train)\n",
    "X_test_anew  = vectorizer.transform(clean_test)\n",
    "X_val_anew = vectorizer.transform(clean_val)\n",
    "xanew_t = np.array(xanew.T) #correcting shape for function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anew(arr):\n",
    "\n",
    "    valence = lambda x : x * xanew_t[0]\n",
    "    arousal = lambda x : x * xanew_t[1]\n",
    "\n",
    "    X_train_valence = np.apply_along_axis(valence, 1, arr.toarray())\n",
    "    X_train_arousal = np.apply_along_axis(arousal, 1, arr.toarray())\n",
    "\n",
    "    return X_train_valence, X_train_arousal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valence, X_train_arousal = get_anew(X_train_anew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on ANEW vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.6821856759845101\n",
      "Explained variance: 0.617935732258481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pca_v = PCA(n_components = 100)\n",
    "pca_a = PCA(n_components = 100)\n",
    "pca_v.fit(X_train_valence)\n",
    "pca_a.fit(X_train_arousal)\n",
    "pca_v_train = pca_v.transform(X_train_valence)\n",
    "pca_a_train = pca_a.transform(X_train_arousal)\n",
    "print('Explained variance:', np.sum(pca_v.explained_variance_ratio_))\n",
    "print('Explained variance:', np.sum(pca_a.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_anew = np.concatenate((pca_v_train, pca_a_train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_valence, X_val_arousal = get_anew(X_val_anew)\n",
    "X_test_valence, X_test_arousal = get_anew(X_test_anew)\n",
    "pca_v_test = pca_v.transform(X_test_valence)\n",
    "pca_a_test = pca_a.transform(X_test_arousal)\n",
    "pca_v_val = pca_v.transform(X_val_valence)\n",
    "pca_a_val = pca_v.transform(X_val_arousal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_anew_val = np.concatenate((pca_v_val, pca_a_val), axis = 1)\n",
    "pca_anew_test = np.concatenate((pca_v_test, pca_a_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.concatenate([X_train_vader, pca_lyrics_train, pca_anew], axis = 1)\n",
    "X_test_all = np.concatenate([X_test_vader, pca_lyrics_test, pca_anew_test], axis = 1)\n",
    "X_val_all = np.concatenate([X_val_vader, pca_lyrics_val, pca_anew_val], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_index = ['pos', 'neg', 'neu', 'compound']\n",
    "\n",
    "for i in range(1,101):\n",
    "    features_index.append(f'tfidf_pca_{i}')\n",
    "for n in range(1,201):\n",
    "    features_index.append(f'anew_pca_{n}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train_all, columns=features_index)\n",
    "test_df = pd.DataFrame(X_test_all, columns=features_index)\n",
    "val_df = pd.DataFrame(X_val_all, columns=features_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('lyrics_features_train.csv', sep = ',')\n",
    "test_df.to_csv('lyrics_features_test.csv', sep = ',')\n",
    "val_df.to_csv('lyrics_features_val.csv', sep = ',')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train = pd.read_csv('lyrics_features_train.csv',delimiter = ',')\n",
    "lyrics_test = pd.read_csv('lyrics_features_test.csv',delimiter = ',')\n",
    "lyrics_val = pd.read_csv('lyrics_features_val.csv',delimiter = ',')\n",
    "\n",
    "data = pd.read_csv('merged_cleaned_sentiment_train.csv', delimiter = ',')\n",
    "data_val = pd.read_csv('merged_cleaned_sentiment_validation.csv', delimiter = ',')\n",
    "data_test = pd.read_csv('merged_cleaned_sentiment_test.csv', delimiter = ',')\n",
    "\n",
    "data = pd.concat([data, pd.get_dummies(data.key, drop_first = True, prefix = 'key')], axis=1)\n",
    "data_val = pd.concat([data_val, pd.get_dummies(data_val.key, drop_first = True, prefix = 'key')], axis=1)\n",
    "data_test = pd.concat([data_test, pd.get_dummies(data_test.key, drop_first = True, prefix = 'key')], axis=1)\n",
    "\n",
    "data_multi = pd.concat([data, lyrics_train], axis = 1).dropna(axis = 0)\n",
    "data_multi_val = pd.concat([data_val, lyrics_val], axis = 1).dropna(axis = 0)\n",
    "data_multi_test = pd.concat([data_test, lyrics_test], axis = 1).dropna(axis = 0)\n",
    "\n",
    "data_labels = data_multi[['y_valence', 'y_arousal']]\n",
    "data_labels_val = data_multi_val[['y_valence', 'y_arousal']]\n",
    "data_labels_test = data_multi_test[['y_valence', 'y_arousal']]\n",
    "\n",
    "data_multi = data_multi.drop(columns = ['Unnamed: 0', 'artist', 'trackname', 'id', 'lyrics','lyrics_cleaned', 'neg', 'neu', 'pos', 'compound' ,'y_valence', 'y_arousal'])\n",
    "data_multi_val = data_multi_val.drop(columns = ['Unnamed: 0', 'artist', 'trackname', 'id', 'lyrics','lyrics_cleaned', 'neg', 'neu', 'pos', 'compound', 'y_valence', 'y_arousal'])\n",
    "data_multi_test = data_multi_test.drop(columns = ['Unnamed: 0', 'artist', 'trackname', 'id', 'lyrics','lyrics_cleaned', 'neg', 'neu', 'pos', 'compound', 'y_valence', 'y_arousal'])\n",
    "\n",
    "X_train = data_multi.to_numpy().astype(np.float32)\n",
    "X_test = data_multi_test.to_numpy().astype(np.float32)\n",
    "X_val = data_multi_val.to_numpy().astype(np.float32)\n",
    "y_train = data_labels.to_numpy().astype(np.float32)\n",
    "y_test = data_labels_test.to_numpy().astype(np.float32)\n",
    "y_val = data_labels_val.to_numpy().astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae83aaff4c47202ead0cb5b0cfe74444d437ac818c9c2cd6826845dc75a11708"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
