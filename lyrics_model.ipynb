{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Lyrics Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('merged_cleaned_sentiment_train.csv', delimiter = ',')\n",
    "data_test = pd.read_csv('merged_cleaned_sentiment_test.csv', delimiter = ',')\n",
    "data_val = pd.read_csv('merged_cleaned_sentiment_validation.csv', delimiter = ',')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vader Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features = data[['pos','neg','neu','compound']]\n",
    "data_labels = data[['y_valence', 'y_arousal']]\n",
    "\n",
    "data_features_test = data_test[['pos','neg','neu','compound']]\n",
    "data_labels_test = data_test[['y_valence', 'y_arousal']]\n",
    "\n",
    "data_features_val = data_val[['pos','neg','neu','compound']]\n",
    "data_labels_val = data_val[['y_valence', 'y_arousal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train_vader, y_train = np.array(data_features), np.array(data_labels)\n",
    "X_test_vader, y_test = np.array(data_features_test), np.array(data_labels_test)\n",
    "X_val_vader, y_val = np.array(data_features_val), np.array(data_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9816597264467664\n",
      "0.022844544146133294\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train_vader,y_train)\n",
    "prediction = reg.predict(X_val_vader)\n",
    "print(mean_squared_error(y_val, prediction, squared = False))\n",
    "print(r2_score(y_val, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very high RMSE considering our predicted values go from -1 to 1. The RMSE is half of the range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lyrics_train = np.array(data['lyrics_cleaned'])\n",
    "data_lyrics_test = np.array(data_test['lyrics_cleaned'])\n",
    "data_lyrics_val = np.array(data_val['lyrics_cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up the lyrics texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data): \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_data = np.full((data.shape), None)\n",
    "\n",
    "    for n, lyrics in enumerate(data):\n",
    "        lyrics = [word.lower().strip(string.punctuation) for word in lyrics.split()]\n",
    "        lyrics = [lemmatizer.lemmatize(word) for word in lyrics]\n",
    "        clean_data[n] = ' '.join(lyrics)\n",
    "\n",
    "    return clean_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = preprocessing(data_lyrics_train)\n",
    "clean_test = preprocessing(data_lyrics_test)\n",
    "clean_val = preprocessing(data_lyrics_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating TF-IDF Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer= 'word', ngram_range = (1,1))\n",
    "X_train_tfidf = vectorizer.fit_transform(clean_train)\n",
    "X_test_tfidf = vectorizer.transform(clean_test)\n",
    "X_val_tfidf = vectorizer.transform(clean_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2558, 26621)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.26271753604112447\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 100)\n",
    "pca.fit(X_train_tfidf.toarray().astype(float))\n",
    "pca_lyrics_train = pca.transform(X_train_tfidf.toarray().astype(float))\n",
    "pca_lyrics_test = pca.transform(X_test_tfidf.toarray().astype(float))\n",
    "pca_lyrics_val = pca.transform(X_val_tfidf.toarray().astype(float))\n",
    "print('Explained variance:', np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANEW Count Features - lexical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repo.clone_from('https://github.com/JULIELab/X-ANEW.git', Path.home()/'Documents/GitHub/CogSci2-Spotify/anew')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "xanew = pd.read_csv('anew/Ratings_Warriner_et_al.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xanew = xanew[['Word', 'V.Mean.Sum', 'A.Mean.Sum']]\n",
    "xanew.columns = ['word', 'valence', 'arousal']\n",
    "xanew.set_index('word', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewords = [str(el) for el in list(xanew.index)]\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary = ewords)\n",
    "X_train_anew = vectorizer.transform(clean_train)\n",
    "X_test_anew  = vectorizer.transform(clean_test)\n",
    "X_val_anew = vectorizer.transform(clean_val)\n",
    "xanew_t = np.array(xanew.T) #correcting shape for function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anew(arr):\n",
    "\n",
    "    valence = lambda x : x * xanew_t[0]\n",
    "    arousal = lambda x : x * xanew_t[1]\n",
    "\n",
    "    X_train_valence = np.apply_along_axis(valence, 1, arr.toarray())\n",
    "    X_train_arousal = np.apply_along_axis(arousal, 1, arr.toarray())\n",
    "\n",
    "    return X_train_valence, X_train_arousal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valence, X_train_arousal = get_anew(X_train_anew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA on ANEW vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.6821856759845101\n",
      "Explained variance: 0.617935732258481\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pca_v = PCA(n_components = 100)\n",
    "pca_a = PCA(n_components = 100)\n",
    "pca_v.fit(X_train_valence)\n",
    "pca_a.fit(X_train_arousal)\n",
    "pca_v_train = pca_v.transform(X_train_valence)\n",
    "pca_a_train = pca_a.transform(X_train_arousal)\n",
    "print('Explained variance:', np.sum(pca_v.explained_variance_ratio_))\n",
    "print('Explained variance:', np.sum(pca_a.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_anew = np.concatenate((pca_v_train, pca_a_train), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_valence, X_val_arousal = get_anew(X_val_anew)\n",
    "X_test_valence, X_test_arousal = get_anew(X_test_anew)\n",
    "pca_v_test = pca_v.transform(X_test_valence)\n",
    "pca_a_test = pca_a.transform(X_test_arousal)\n",
    "pca_v_val = pca_v.transform(X_val_valence)\n",
    "pca_a_val = pca_v.transform(X_val_arousal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_anew_val = np.concatenate((pca_v_val, pca_a_val), axis = 1)\n",
    "pca_anew_test = np.concatenate((pca_v_test, pca_a_test), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = np.concatenate([X_train_vader, pca_lyrics_train, pca_anew], axis = 1)\n",
    "X_test_all = np.concatenate([X_test_vader, pca_lyrics_test, pca_anew_test], axis = 1)\n",
    "X_val_all = np.concatenate([X_val_vader, pca_lyrics_val, pca_anew_val], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_index = ['pos', 'neg', 'neu', 'compound']\n",
    "\n",
    "for i in range(1,101):\n",
    "    features_index.append(f'tfidf_pca_{i}')\n",
    "for n in range(1,201):\n",
    "    features_index.append(f'anew_pca_{n}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(X_train_all, columns=features_index)\n",
    "test_df = pd.DataFrame(X_test_all, columns=features_index)\n",
    "val_df = pd.DataFrame(X_val_all, columns=features_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('lyrics_features_train.csv', sep = ',')\n",
    "test_df.to_csv('lyrics_features_test.csv', sep = ',')\n",
    "val_df.to_csv('lyrics_features_val.csv', sep = ',')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_train = pd.read_csv('lyrics_features_train.csv',delimiter = ',')\n",
    "lyrics_test = pd.read_csv('lyrics_features_test.csv',delimiter = ',')\n",
    "lyrics_val = pd.read_csv('lyrics_features_val.csv',delimiter = ',')\n",
    "\n",
    "data = pd.read_csv('merged_cleaned_sentiment_train.csv', delimiter = ',')\n",
    "data_val = pd.read_csv('merged_cleaned_sentiment_validation.csv', delimiter = ',')\n",
    "data_test = pd.read_csv('merged_cleaned_sentiment_test.csv', delimiter = ',')\n",
    "\n",
    "data = pd.concat([data, pd.get_dummies(data.key, drop_first = True, prefix = 'key')], axis=1)\n",
    "data_val = pd.concat([data_val, pd.get_dummies(data_val.key, drop_first = True, prefix = 'key')], axis=1)\n",
    "data_test = pd.concat([data_test, pd.get_dummies(data_test.key, drop_first = True, prefix = 'key')], axis=1)\n",
    "\n",
    "data_multi = pd.concat([data, lyrics_train], axis = 1).dropna(axis = 0)\n",
    "data_multi_val = pd.concat([data_val, lyrics_val], axis = 1).dropna(axis = 0)\n",
    "data_multi_test = pd.concat([data_test, lyrics_test], axis = 1).dropna(axis = 0)\n",
    "\n",
    "data_labels = data_multi[['y_valence', 'y_arousal']]\n",
    "data_labels_val = data_multi_val[['y_valence', 'y_arousal']]\n",
    "data_labels_test = data_multi_test[['y_valence', 'y_arousal']]\n",
    "\n",
    "data_multi = data_multi.drop(columns = ['Unnamed: 0', 'artist', 'trackname', 'id', 'lyrics','lyrics_cleaned', 'neg', 'neu', 'pos', 'compound' ,'y_valence', 'y_arousal'])\n",
    "data_multi_val = data_multi_val.drop(columns = ['Unnamed: 0', 'artist', 'trackname', 'id', 'lyrics','lyrics_cleaned', 'neg', 'neu', 'pos', 'compound', 'y_valence', 'y_arousal'])\n",
    "data_multi_test = data_multi_test.drop(columns = ['Unnamed: 0', 'artist', 'trackname', 'id', 'lyrics','lyrics_cleaned', 'neg', 'neu', 'pos', 'compound', 'y_valence', 'y_arousal'])\n",
    "\n",
    "X_train = data_multi.to_numpy().astype(np.float32)\n",
    "X_test = data_multi_test.to_numpy().astype(np.float32)\n",
    "X_val = data_multi_val.to_numpy().astype(np.float32)\n",
    "y_train = data_labels.to_numpy().astype(np.float32)\n",
    "y_test = data_labels_test.to_numpy().astype(np.float32)\n",
    "y_val = data_labels_val.to_numpy().astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae83aaff4c47202ead0cb5b0cfe74444d437ac818c9c2cd6826845dc75a11708"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
